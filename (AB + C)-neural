import numpy as np
#AB+C, odd numbers of 1 is =1
learn = 2
X = [[0,0,0], [0,0,1], [0,1,0], [0,1,1], [1,0,0], [1,0,1], [1,1,1]] #6 is missing
Y = [[0], [1], [0], [1],[0],[1],[1]]

X = np.asarray(X).T #(3*7)
Y = np.asarray(Y).T

w = np.random.randn(1,3)*0.01 #(1*3)
b = 0
def sigmoid(z):
    func = 1/(1+np.exp(-z))
    return func
def propogate(X,Y,w,b): #this is correct
    z = np.dot(w,X) +b
    a = sigmoid(z)
    
    cost = -np.sum((Y*np.log(a) + (1-Y)*np.log(1-a)))/X.shape[1]
    dw = (np.dot(X,(a-Y).T))/X.shape[1]
    
    db = np.sum(a-Y)/X.shape[1]
    
    
    grads = {"dw":dw , "db":db}
    return (grads,cost) #(1*7) , each coloumn for 1 number

def descent(w,b,X,Y,learn):
    for x in range(1000):
        
        grads,cost = propogate(X,Y,w,b)
        
        dw = grads['dw']
        db = grads['db']
        w = w - (learn*dw.T)
        b = b - (learn*db)
        
        params = {"w": w,"b": b}
        grads = {"dw": dw,"db": db}
    return params, grads

def predict(w, b, X):
    m = X.shape[1]
    Y_prediction = np.zeros((1,m))
    #w = w.reshape(X.shape[0], 1)
    A = sigmoid(np.dot(w,X)+b)
    print(A)
    #print('')
    for i in range(A.shape[1]):
        if(A[0][i]<=0.5):
            A[0][i]=0
        else:
            A[0][i]=1
        Y_prediction=A
    return Y_prediction
parameters, grads = descent(w,b,X,Y,learn)
w = parameters["w"]
b = parameters["b"]
pred = predict(w,b,X)
print(pred)
